"""
ë„¤ì´ë²„ API ì„œë¹„ìŠ¤

ë„¤ì´ë²„ ê²€ìƒ‰ APIë¥¼ í†µí•œ ë¸”ë¡œê·¸ ê²€ìƒ‰ ë° ì¥ì†Œ ì •ë³´ ìˆ˜ì§‘
"""

import os
import aiohttp
import asyncio
from typing import Dict, Any, List
from bs4 import BeautifulSoup
from app.services.ssl_helper import create_http_session

class NaverService:
    def __init__(self):
        self.client_id = os.getenv("NAVER_CLIENT_ID")
        self.client_secret = os.getenv("NAVER_CLIENT_SECRET")
        self.base_url = "https://openapi.naver.com/v1"
    
    async def search_blogs(self, query: str, display: int = 5) -> List[Dict[str, Any]]:
        """ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰"""
        if not self.client_id or not self.client_secret:
            print(f"âš ï¸ Naver API í‚¤ ì—†ìŒ â†’ Mock ë°ì´í„° ë°˜í™˜")
            return self._mock_blog_results(query)
        
        headers = {
            "X-Naver-Client-Id": self.client_id,
            "X-Naver-Client-Secret": self.client_secret
        }
        
        params = {
            "query": query,
            "display": display,
            "sort": "sim"  # ì •í™•ë„ìˆœ
        }
        
        try:
            print(f"ğŸ“¡ Naver Blog API í˜¸ì¶œ: '{query}' (display={display})")
            async with create_http_session() as session:
                async with session.get(
                    f"{self.base_url}/search/blog.json",
                    headers=headers,
                    params=params
                ) as response:
                    print(f"   ì‘ë‹µ ìƒíƒœ: {response.status}")
                    if response.status == 200:
                        data = await response.json()
                        items = data.get("items", [])
                        print(f"   âœ… API ì‘ë‹µ: {len(items)}ê°œ ë¸”ë¡œê·¸ ê²€ìƒ‰ë¨")
                        if items:
                            # ì²« ë²ˆì§¸ ì•„ì´í…œì˜ êµ¬ì¡° í™•ì¸
                            first_item = items[0]
                            print(f"   ğŸ” ì²« ë²ˆì§¸ ì•„ì´í…œ êµ¬ì¡°:")
                            print(f"      - title: {first_item.get('title', 'N/A')[:50]}")
                            print(f"      - link: {first_item.get('link', 'âŒ ì—†ìŒ')[:80]}")
                            print(f"      - bloggername: {first_item.get('bloggername', 'N/A')}")
                        return await self._process_blog_results(items)
                    else:
                        print(f"   âŒ API ì˜¤ë¥˜ â†’ Mock ë°ì´í„° ë°˜í™˜")
                        return self._mock_blog_results(query)
        except Exception as e:
            print(f"âŒ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
            import traceback
            traceback.print_exc()
            return self._mock_blog_results(query)
    
    async def search_places(self, query: str, display: int = 5) -> List[Dict[str, Any]]:
        """ë„¤ì´ë²„ ì§€ì—­ ê²€ìƒ‰"""
        if not self.client_id or not self.client_secret:
            return self._mock_place_results(query)
        
        headers = {
            "X-Naver-Client-Id": self.client_id,
            "X-Naver-Client-Secret": self.client_secret
        }
        
        params = {
            "query": query,
            "display": display
        }
        
        try:
            async with create_http_session() as session:
                async with session.get(
                    f"{self.base_url}/search/local.json",
                    headers=headers,
                    params=params
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        return self._process_place_results(data.get("items", []))
                    else:
                        return self._mock_place_results(query)
        except Exception as e:
            print(f"ë„¤ì´ë²„ ì§€ì—­ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
            return self._mock_place_results(query)
    
    async def _process_blog_results(self, items: List[Dict]) -> List[Dict[str, Any]]:
        """ë¸”ë¡œê·¸ ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬ ë° ìƒì„¸ ë‚´ìš© ë¶„ì„"""
        processed_results = []
        
        for item in items:
            # ë¸”ë¡œê·¸ ìƒì„¸ ë‚´ìš© ë¶„ì„
            blog_link = item.get("link", "")
            detailed_content = await self._get_blog_summary(blog_link)
            
            blog_info = {
                "title": self._clean_html(item.get("title", "")),
                "description": self._clean_html(item.get("description", "")),
                "link": blog_link,
                "url": blog_link,
                "blogger": item.get("bloggername", ""),
                "date": item.get("postdate", ""),
                "content_analysis": detailed_content if isinstance(detailed_content, dict) else {"summary": detailed_content},
                "summary": detailed_content.get("summary", detailed_content) if isinstance(detailed_content, dict) else detailed_content
            }
            processed_results.append(blog_info)
        
        return processed_results
    
    async def _get_blog_summary(self, url: str) -> str:
        """ë¸”ë¡œê·¸ ë‚´ìš© ìš”ì•½ (ì‹¤ì œ ë‚´ìš© í¬ë¡¤ë§)"""
        if not self._is_safe_url(url):
            return "ì•ˆì „í•˜ì§€ ì•Šì€ URLì…ë‹ˆë‹¤."
            
        try:
            async with create_http_session() as session:
                async with session.get(url, timeout=10, headers={
                    'User-Agent': 'Mozilla/5.0 (compatible; TravelBot/1.0)'
                }) as response:
                    if response.status == 200:
                        html = await response.text()
                        return self._extract_detailed_blog_content(html)
        except Exception as e:
            print(f"ë¸”ë¡œê·¸ í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")
        
        return {"summary": "ë¸”ë¡œê·¸ ë‚´ìš©ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "keywords": [], "rating": 0, "sentiment": "ì¤‘ë¦½ì ", "highlights": []}
    
    def _extract_detailed_blog_content(self, html: str) -> Dict[str, Any]:
        """ë„¤ì´ë²„ ë¸”ë¡œê·¸ ìƒì„¸ ë‚´ìš© ì¶”ì¶œ ë° ë¶„ì„"""
        soup = BeautifulSoup(html, 'html.parser')
        
        # ë„¤ì´ë²„ ë¸”ë¡œê·¸ íŠ¹í™” ì„ íƒì
        selectors = [
            '.se-main-container',  # ìŠ¤ë§ˆíŠ¸ì—ë””í„°
            '.post-view',          # ì¼ë°˜ ë¸”ë¡œê·¸
            '#postViewArea',       # êµ¬ë²„ì „
            '.blog-content'        # ê¸°íƒ€
        ]
        
        content = ""
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                content = element.get_text(strip=True)
                break
        
        if not content:
            content = soup.get_text(strip=True)
        
        # ë‚´ìš© ì •ë¦¬
        import re
        content = re.sub(r'\s+', ' ', content)
        
        # ìƒì„¸ ë¶„ì„
        analysis = self._analyze_blog_content(content)
        
        return {
            "content": content[:500] + "..." if len(content) > 500 else content,
            "keywords": analysis["keywords"],
            "rating": analysis["rating"],
            "sentiment": analysis["sentiment"],
            "highlights": analysis["highlights"],
            "summary": analysis["summary"]
        }
    
    def _analyze_blog_content(self, content: str) -> Dict[str, Any]:
        """ë¸”ë¡œê·¸ ë‚´ìš© ìƒì„¸ ë¶„ì„"""
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = self._extract_keywords(content)
        
        # í‰ì  ì¶”ì¶œ
        rating = self._extract_rating(content)
        
        # ê°ì • ë¶„ì„
        sentiment = self._analyze_sentiment(content)
        
        # í•˜ì´ë¼ì´íŠ¸ ì¶”ì¶œ
        highlights = self._extract_highlights(content)
        
        # ìš”ì•½ ìƒì„±
        summary = content[:200] + "..." if len(content) > 200 else content
        
        return {
            "keywords": keywords,
            "rating": rating,
            "sentiment": sentiment,
            "highlights": highlights,
            "summary": summary
        }
    
    def _extract_rating(self, content: str) -> float:
        """ë‚´ìš©ì—ì„œ í‰ì  ì¶”ì¶œ"""
        import re
        
        # í‰ì  íŒ¨í„´ ì°¾ê¸°
        rating_patterns = [
            r'(\d+)ì ',
            r'â˜…+',
            r'â­+',
            r'(\d+)/5',
            r'(\d+)/10'
        ]
        
        for pattern in rating_patterns:
            matches = re.findall(pattern, content)
            if matches:
                if pattern in ['â˜…+', 'â­+']:
                    return len(matches[0])
                else:
                    try:
                        return float(matches[0])
                    except:
                        continue
        
        # ê¸ì •/ë¶€ì • í‚¤ì›Œë“œë¡œ ì¶”ì • í‰ì 
        positive_count = len([w for w in ['ë§›ìˆ', 'ì¢‹', 'ì¶”ì²œ', 'ë§Œì¡±'] if w in content])
        negative_count = len([w for w in ['ë³„ë¡œ', 'ì•„ì‰¬', 'ì‹¤ë§'] if w in content])
        
        if positive_count > negative_count:
            return 4.0 + (positive_count * 0.2)
        elif negative_count > positive_count:
            return 3.0 - (negative_count * 0.3)
        else:
            return 3.5
    
    def _analyze_sentiment(self, content: str) -> str:
        """ê°ì • ë¶„ì„"""
        positive_words = ['ë§›ìˆ', 'ì¢‹', 'ì¶”ì²œ', 'ë§Œì¡±', 'í›Œë¥­', 'ìµœê³ ']
        negative_words = ['ë³„ë¡œ', 'ì•„ì‰¬', 'ì‹¤ë§', 'ë¶ˆì¹œì ˆ', 'ë¹„ì‹¸']
        
        positive_count = sum(1 for word in positive_words if word in content)
        negative_count = sum(1 for word in negative_words if word in content)
        
        if positive_count > negative_count * 2:
            return "ë§¤ìš° ê¸ì •ì "
        elif positive_count > negative_count:
            return "ê¸ì •ì "
        elif negative_count > positive_count:
            return "ë¶€ì •ì "
        else:
            return "ì¤‘ë¦½ì "
    
    def _extract_highlights(self, content: str) -> List[str]:
        """ì£¼ìš” ë‚´ìš© í•˜ì´ë¼ì´íŠ¸ ì¶”ì¶œ"""
        import re
        
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
        sentences = re.split(r'[.!?]', content)
        
        highlights = []
        highlight_keywords = ['ë§›ìˆ', 'ì¶”ì²œ', 'ì¢‹', 'ìµœê³ ', 'íŠ¹ë³„', 'ì¸ìƒì ', 'ê¸°ì–µì— ë‚¨']
        
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) > 10 and any(keyword in sentence for keyword in highlight_keywords):
                highlights.append(sentence[:100] + "..." if len(sentence) > 100 else sentence)
                if len(highlights) >= 3:  # ìµœëŒ€ 3ê°œê¹Œì§€
                    break
        
        return highlights
    
    def _extract_keywords(self, content: str) -> list:
        """ì—¬í–‰/ë§›ì§‘ ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        positive_words = ['ë§›ìˆ', 'ì¢‹', 'ì¶”ì²œ', 'ë§Œì¡±', 'í›Œë¥­', 'ìµœê³ ', 'ê¹”ë”', 'ì¹œì ˆ', 'ë¶„ìœ„ê¸°', 'ê°€ì„±ë¹„']
        negative_words = ['ë³„ë¡œ', 'ì•„ì‰¬', 'ì‹¤ë§', 'ë¶ˆì¹œì ˆ', 'ë¹„ì‹¸', 'ë§›ì—†']
        
        found_keywords = []
        for word in positive_words + negative_words:
            if word in content:
                found_keywords.append(word)
        
        return found_keywords[:5]
    
    def _is_safe_url(self, url: str) -> bool:
        """ì•ˆì „í•œ URL ê²€ì¦"""
        from urllib.parse import urlparse
        try:
            parsed = urlparse(url)
            allowed_domains = ['blog.naver.com', 'post.naver.com', 'cafe.naver.com']
            return any(domain in parsed.netloc.lower() for domain in allowed_domains)
        except:
            return False
    
    def _process_place_results(self, items: List[Dict]) -> List[Dict[str, Any]]:
        """ì¥ì†Œ ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬"""
        processed_results = []
        
        # ğŸ” ì²« ë²ˆì§¸ ì•„ì´í…œ ì›ë³¸ ì‘ë‹µ ë””ë²„ê¹…
        if items and len(items) > 0:
            print(f"         ğŸ” [NaverService] ì›ë³¸ API ì‘ë‹µ ìƒ˜í”Œ:")
            print(f"            Keys: {list(items[0].keys())}")
            print(f"            title: {items[0].get('title')}")
            print(f"            mapy: {items[0].get('mapy')} (type: {type(items[0].get('mapy'))})")
            print(f"            mapx: {items[0].get('mapx')} (type: {type(items[0].get('mapx'))})")
        
        for item in items:
            mapy = item.get("mapy")
            mapx = item.get("mapx")
            
            # ì¢Œí‘œ ë³€í™˜
            lat = float(mapy) / 10000000 if mapy else None
            lng = float(mapx) / 10000000 if mapx else None
            
            place_info = {
                "name": self._clean_html(item.get("title", "")),
                "address": item.get("address", ""),
                "road_address": item.get("roadAddress", ""),
                "phone": item.get("telephone", ""),
                "category": item.get("category", ""),
                "description": self._clean_html(item.get("description", "")),
                "lat": lat,
                "lng": lng
            }
            processed_results.append(place_info)
        
        return processed_results
    
    def _clean_html(self, text: str) -> str:
        """HTML íƒœê·¸ ì œê±°"""
        import re
        return re.sub(r'<[^>]+>', '', text)
    
    def _mock_blog_results(self, query: str) -> List[Dict[str, Any]]:
        """API í‚¤ê°€ ì—†ì„ ë•Œ ëª¨ì˜ ë¸”ë¡œê·¸ ê²°ê³¼"""
        import urllib.parse
        encoded_query = urllib.parse.quote(f"{query} í›„ê¸°")
        
        return [
            {
                "title": f"{query} í›„ê¸° - ë„¤ì´ë²„ ë¸”ë¡œê·¸",
                "description": f"{query}ì— ëŒ€í•œ ìƒì„¸í•œ í›„ê¸°ì™€ íŒì„ ê³µìœ í•©ë‹ˆë‹¤.",
                "link": f"https://search.naver.com/search.naver?where=blog&query={encoded_query}",
                "blogger": "ì—¬í–‰ëŸ¬ë²„",
                "date": "20241201",
                "summary": f"{query}ëŠ” ì •ë§ ì¢‹ì€ ê³³ì´ì—ˆìŠµë‹ˆë‹¤. íŠ¹íˆ ìŒì‹ì´ ë§›ìˆê³  ë¶„ìœ„ê¸°ê°€ ì¢‹ì•„ì„œ ì¶”ì²œí•©ë‹ˆë‹¤."
            },
            {
                "title": f"{query} ë§›ì§‘ ë¦¬ë·°",
                "description": f"{query} ì‹¤ì œ ë°©ë¬¸ í›„ê¸°ì…ë‹ˆë‹¤.",
                "link": f"https://search.naver.com/search.naver?where=blog&query={encoded_query}",
                "blogger": "ë§›ì§‘íƒí—˜ê°€",
                "date": "20241130",
                "summary": f"ì§ì ‘ ê°€ë³¸ {query} ì†”ì§í•œ í›„ê¸°. ì‚¬ì§„ê³¼ í•¨ê»˜ ìƒì„¸í•œ ë¦¬ë·°ë¥¼ ë‚¨ê¹ë‹ˆë‹¤."
            },
            {
                "title": f"{query} ë°©ë¬¸ í›„ê¸°",
                "description": f"{query} ì§ì ‘ ë°©ë¬¸í•œ ì†”ì§í•œ í›„ê¸°ì…ë‹ˆë‹¤.",
                "link": f"https://blog.naver.com/PostSearchList.naver?blogId=&from=postList&query={encoded_query}",
                "blogger": "ë§›ì§‘ë§¤ë‹ˆì•„",
                "date": "20241129",
                "summary": f"{query} ì§ì ‘ ê°€ë³¸ í›„ê¸°. ì‚¬ì§„ ë§ì´ ì°ì–´ì™”ì–´ìš”. ì •ë§ ì¶”ì²œí•©ë‹ˆë‹¤!"
            }
        ]
    
    def _mock_place_results(self, query: str) -> List[Dict[str, Any]]:
        """API í‚¤ê°€ ì—†ì„ ë•Œ ëª¨ì˜ ì¥ì†Œ ê²°ê³¼"""
        return [
            {
                "name": query,
                "address": "ì„œìš¸ì‹œ ê°•ë‚¨êµ¬",
                "road_address": "ì„œìš¸ì‹œ ê°•ë‚¨êµ¬ í…Œí—¤ë€ë¡œ 123",
                "phone": "02-1234-5678",
                "category": "ìŒì‹ì ",
                "description": f"{query}ì— ëŒ€í•œ ì •ë³´ì…ë‹ˆë‹¤.",
                "lat": 37.5663,
                "lng": 126.8247
            }
        ]